#!/bin/sh

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#enable checking: if a command exits with an error and the caller does not check such error, the script aborts immediately. 
set -e

# Create a Hbase AMI. Runs on the EC2 instance.

# Import variables
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`

arch=$ARCH
[ -z "$arch" ] && arch=$SLAVE_ARCH

echo "Remote: ARCH is $arch"
echo "Hadoop version: $HADOOP_VERSION"
echo "HBase version: $HBASE_VERSION"

if [ -f /mnt/credentials.sh ] ; then
  . /mnt/credentials.sh &&
    echo "Imported credentials" &&
    echo "  AWS_ACCOUNT_ID: $AWS_ACCOUNT_ID"
fi

# Install Java
# Perform any URL substitutions that must be done at this late stage
JAVA_URL=`echo $JAVA_URL | sed -e "s/@arch@/$arch/g"`
echo "Downloading and installing java binary."
cd /usr/local
wget -nv -O java.bin $JAVA_URL
sh java.bin
rm -f java.bin
ln -s `find /usr/local -type d -maxdepth 1 -type d -name "jdk*" | head -n 1 ` /usr/local/jdk

echo "Downloading and installing Jruby."
wget -nv http://ekoontz-tarballs.s3.amazonaws.com/jruby-bin-1.5.1.tar.gz
tar --directory=/usr/local/ -x -z -f jruby-bin-1.5.1.tar.gz
ln -s /usr/local/jruby-1.5.1 /usr/local/jruby
echo "Done."

# Install tools
echo "Installing rpms."
yum -y update
yum -y install lynx screen ganglia-gmetad ganglia-gmond ganglia-web httpd php lzo-devel xfsprogs emacs-nox krb5-server
yum -y clean all
chkconfig --levels 0123456 httpd off
chkconfig --levels 0123456 gmetad off
chkconfig --levels 0123456 gmond off
chkconfig --levels 0123456 krb5kdc off
chkconfig --levels 0123456 kadmin off

# Install Hadoop
echo "Installing Hadoop $HADOOP_VERSION."
cd /usr/local
wget -nv $HADOOP_URL
tar xzf hadoop-$HADOOP_VERSION.tar.gz
rm -f hadoop-$HADOOP_VERSION.tar.gz

# Install HBase
echo "Installing HBase $HBASE_VERSION."
cd /usr/local
wget -nv $HBASE_URL
tar xzf hbase-$HBASE_VERSION.tar.gz 
rm -f hbase-$HBASE_VERSION.tar.gz

# Configure Hadoop
echo "export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}
export HADOOP_LOG_DIR=/mnt/hadoop/logs
export HADOOP_SLAVE_SLEEP=1
export HADOOP_OPTS=-server" >> /usr/local/hadoop-$HADOOP_VERSION/conf/hadoop-env.sh

# Configure HBase
echo "export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}
export HBASE_OPTS=\"$HBASE_OPTS -server -XX:+HeapDumpOnOutOfMemoryError\"
export HBASE_LOG_DIR=/mnt/hbase/logs
export HBASE_SLAVE_SLEEP=1" >> /usr/local/hbase-$HBASE_VERSION/conf/hbase-env.sh

# Run user data as script on instance startup
chmod +x /etc/init.d/ec2-run-user-data
echo "/etc/init.d/ec2-run-user-data" >> /etc/rc.d/rc.local

# Setup root user bash environment

echo "export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}" >> /root/.bash_profile
echo "export HADOOP_HOME=/usr/local/hadoop-${HADOOP_VERSION}" >> /root/.bash_profile
echo "export HBASE_HOME=/usr/local/hbase-${HBASE_VERSION}" >> /root/.bash_profile
echo 'export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH' >> /root/.bash_profile

# Configure networking.
# Delete SSH authorized_keys since it includes the key it was launched with. (Note that it is re-populated when an instance starts.)
rm -f /root/.ssh/authorized_keys

# Ensure logging in to new hosts is seamless.
echo '    StrictHostKeyChecking no' >> /etc/ssh/ssh_config

# Install LZO
echo "Installing LZO codec support"
wget -nv -O /tmp/lzo-linux-${HADOOP_VERSION}.tar.gz $LZO_URL
cd /usr/local/hadoop-${HADOOP_VERSION} && tar xzf /tmp/lzo-linux-${HADOOP_VERSION}.tar.gz
cd /usr/local/hbase-${HBASE_VERSION} && tar xzf /tmp/lzo-linux-${HADOOP_VERSION}.tar.gz
rm -f /tmp/lzo-linux-${HADOOP_VERSION}.tar.gz

#Make symlinks to .jars for portability of automation on this instance.
#[root@domU-12-31-39-0A-34-12 hadoop]# ls -l *.jar
#-rw-rw-r-- 1 root root    6832 Jun 30 18:04 hadoop-ant-0.20-tm-2.jar
#-rw-rw-r-- 1 root root 2717619 Jun 30 18:05 hadoop-core-0.20-tm-2.jar
#lrwxrwxrwx 1 root root      25 Jul  6 23:27 hadoop-core.jar -> hadoop-core-0.20-tm-2.jar
#-rw-rw-r-- 1 root root  142543 Jun 30 18:04 hadoop-examples-0.20-tm-2.jar
#-rw-rw-r-- 1 root root 1644811 Jun 30 18:05 hadoop-test-0.20-tm-2.jar
#lrwxrwxrwx 1 root root      25 Jul  6 22:54 hadoop-test.jar -> hadoop-test-0.20-tm-2.jar
#-rw-rw-r-- 1 root root   69935 Jun 30 18:04 hadoop-tools-0.20-tm-2.jar

# fix above : add all symlinks.
cd /usr/local/hadoop-${HADOOP_VERSION} && ln -s hadoop-core-${HADOOP_VERSION}.jar hadoop-core.jar

# Bundle and upload image
cd ~root
# Don't need to delete .bash_history since it isn't written until exit.
df -h
ec2-bundle-vol -d /mnt -k /mnt/key.pem -c /mnt/cert.pem -u $AWS_ACCOUNT_ID -s 3072 -p hbase-$HBASE_VERSION-$arch -r $arch

ec2-upload-bundle --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY  --bucket $S3_BUCKET --manifest /mnt/hbase-$HBASE_VERSION-$arch.manifest.xml 

# End
echo Done
